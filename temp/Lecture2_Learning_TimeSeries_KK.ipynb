{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습방향과 알고리즘(Learning Style and Algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습방향\n",
    "알고리즘이 문제를 모델링 할 수있는 3 가지 또는 4 가지 방법이 있습니다.  \n",
    "이 분류를 이해하는 목적은 여러분들이 주어진 문제를 해결하기 위한 방향을 정하는 기준을 확인하는 것입니다.  \n",
    "- 문제 해결을 위해 어떤 알고리즘부터 접근할 수 있는지\n",
    "- 입력 데이터는 알고리즘마다 어떻게 준비해야 하는지\n",
    "\n",
    "- 기계학습의 3가지 분류:  \n",
    "<center><img src='Image/ML_Type_Application_Circle.jpg' width='600'></center>  \n",
    "\n",
    "- 기계학습의 4가지 분류:\n",
    "<center><img src='Image/ML_Type_Application.png' width='600'></center>  \n",
    "\n",
    "- 각 분류별 특성과 역할:  \n",
    "<center><img src='Image/ML_Type_Glance.jpg' width='600'></center>\n",
    "\n",
    "| - | Supervised Learning | Unsupervised Learning | Semi-supervised Learning |\n",
    "|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Input Data | labeled | unlabeled | labeled + unlabeled |\n",
    "| Output Result | labeled | unlabeled | labeled + unlabeled |\n",
    "| - | <img src='Image/Supervised-Learning-Algorithms.png' width='150'> | <img src='Image/Unsupervised-Learning-Algorithms.png' width='150'> | <img src='Image/Semi-supervised-Learning-Algorithms.png' width='150'> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습방향에 따른 알고리즘 [(Summary)](https://en.wikipedia.org/wiki/Outline_of_machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "| Regression Algorithms | Instance-based Algorithms | Regularization Algorithms | Decision Tree Algorithms | Bayesian Algorithms | Artificial Neural Network Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Ordinary Least Squares Regression (OLSR) | k-Nearest Neighbor (kNN) | Ridge Regression | Classification and Regression Tree (CART) | Naive Bayes | Perceptron |\n",
    "| Linear Regression | Learning Vector Quantization (LVQ) | Least Absolute Shrinkage and Selection Operator (LASSO) | Iterative Dichotomiser 3 (ID3) | Gaussian Naive Bayes | Back-Propagation |\n",
    "| Logistic Regression | Self-Organizing Map (SOM) | Elastic Net | C4.5 and C5.0 (different versions of a powerful approach) | Multinomial Naive Bayes | Hopfield Network |\n",
    "| Stepwise Regression | Locally Weighted Learning (LWL) | Least-Angle Regression (LARS) | Chi-squared Automatic Interaction Detection (CHAID) | Averaged One-Dependence Estimators (AODE) | Radial Basis Function Network (RBFN) |\n",
    "| Multivariate Adaptive Regression Splines (MARS) | - | - | Decision Stump | Bayesian Belief Network (BBN) | - |\n",
    "| Locally Estimated Scatterplot Smoothing (LOESS) | - | - | M5 | Bayesian Network (BN) | - |\n",
    "| - | - | - | Conditional Decision Trees | - | - |\n",
    "| <img src='Image/Regression-Algorithms.png' width='150'> | <img src='Image/Instance-based-Algorithms.png' width='150'> | <img src='Image/Regularization-Algorithms.png' width='150'> | <img src='Image/Decision-Tree-Algorithms.png' width='150'> | <img src='Image/Bayesian-Algorithms.png' width='150'> | <img src='Image/Artificial-Neural-Network-Algorithms.png' width='150'> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning  \n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) | - | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering | - | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "| - | - | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) | - |\n",
    "| - | - | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) | - |\n",
    "| - | - | Linear Discriminant Analysis (LDA) | Random Forest | - |\n",
    "| - | - | Mixture Discriminant Analysis (MDA) | - | - |\n",
    "| - | - | Quadratic Discriminant Analysis (QDA) | - | - |\n",
    "| - | - | Flexible Discriminant Analysis (FDA) | - | - |\n",
    "| <img src='Image/Clustering-Algorithms.png' width='150'> | <img src='Image/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='Image/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='Image/Ensemble-Algorithms.png' width='150'> | <img src='Image/Deep-Learning-Algorithms.png' width='150'> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열 데이터/분석과 기계학습의 차이\n",
    "\n",
    "- **확률 과정(Stochastic Process)**: 상관 관계를 가지는 무한개의 변수의 순서열  \n",
    "\n",
    "<center>$Y$ = {$\\dots$, $Y_{-2}$, $Y_{-1}$, $Y_{0}$, $Y_{1}$, $Y_{2}$, $\\dots$}\n",
    "and\n",
    "$X$ = {$\\dots$, $X_{-2}$, $X_{-1}$, $X_{0}$, $X_{1}$, $X_{2}$, $\\dots$}</center>\n",
    "\n",
    "<center>$X_1$ = {$\\dots$, $X_{1,-2}$, $X_{1,-1}$, $X_{1,0}$, $X_{1,1}$, $X_{1,2}$, $\\dots$}</center>\n",
    "<center>$X_2$ = {$\\dots$, $X_{2,-2}$, $X_{2,-1}$, $X_{2,0}$, $X_{2,1}$, $X_{2,2}$, $\\dots$}</center>\n",
    "\n",
    "- **시계열 데이터(Time Series Data):** 일정한 시간 간격으로 기록된 확률과정의 샘플\n",
    "\n",
    "<center>$y$ = {$\\dots$, $y_{-2}$, $y_{-1}$, $y_{0}$, $y_{1}$, $y_{2}$, $\\dots$} or {$y_t$ : $t$ = $\\dots$, -2, -1, 0, 1, 2, $\\dots$}</center>\n",
    "\n",
    "<center>$x$ = {$\\dots$, $x_{-2}$, $x_{-1}$, $x_{0}$, $x_{1}$, $x_{2}$, $\\dots$} or {$x_t$ : $t$ = $\\dots$, -2, -1, 0, 1, 2, $\\dots$}\n",
    "</center>\n",
    "\n",
    "<center>$x_1$ = {$\\dots$, $x_{1,-2}$, $x_{1,-1}$, $x_{1,0}$, $x_{1,1}$, $x_{1,2}$, $\\dots$} or {$x_{1t}$ : $t$ = $\\dots$, -2, -1, 0, 1, 2, $\\dots$}</center>\n",
    "<center>$x_2$ = {$\\dots$, $x_{2,-2}$, $x_{2,-1}$, $x_{2,0}$, $x_{2,1}$, $x_{2,2}$, $\\dots$} or {$x_{2t}$ : $t$ = $\\dots$, -2, -1, 0, 1, 2, $\\dots$}</center>\n",
    "\n",
    "> - 독립변수($x_t$)와 알고자 하는 종속변수($y_t$)가 시간단위($t$)를 포함  \n",
    "> - 모델의 출력(Output)은 $y$의 시간 $t$에서의 예측값($\\hat{y_t}$)  \n",
    "> - 기계학습과 시계열예측 간 큰 차이가 존재하기에, 시계열 변수생성은 약간의 조정들을 요구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 독자적인 시계열 성분들 존재\n",
    "\n",
    "> 데이터 과학자로서 이미 수동/자동 변수 생성(Feature Engineering)에 익숙하지만, 신규 변수를 생성하는 것은 분석에서 가장 중요하고 시간이 많이 걸리는 작업 중 하나입니다.\n",
    "\n",
    "- **변수 생성시 주의할 점**  \n",
    "1) 미래의 실제 종속변수 예측값이 어떤 독립/종속변수의 FE에 의해 효과가 있을지 단정할 수 없음  \n",
    "2) 독립변수의 예측값을 FE를 통해 생성될 수 있지만 이는 종속변수의 예측에 오류증가를 야기할 수 있음  \n",
    "\n",
    "\n",
    "- **빈도(Frequency)**: 계절성 패턴(Seasonality)이 나타나기 전까지의 데이터 갯수로 사람이 정해야 함 \n",
    "\n",
    "> 예시: 계설성이 1년에 1회 나타날 경우,  \n",
    "    \n",
    "| Data | frequency |\n",
    "|-----------|-----------|\n",
    "| Annual | 1 |\n",
    "| Quarterly | 4 |\n",
    "| Monthly | 12 |\n",
    "| Weekly | 52 |\n",
    "\n",
    "> 예시: 데이터가 \"일(Day)\" 단위로 수집된 경우,  \n",
    "   \n",
    "| Seasonality | frequency |\n",
    "|-------------|-----------|\n",
    "| Weekly | 7 |\n",
    "| Annual | 365 |\n",
    "\n",
    "> 예시: 데이터가 \"분(Minute)\" 단위로 수집된 경우,  \n",
    "    \n",
    "| Seasonality | frequency |\n",
    "|-------------|-----------|\n",
    "| Hourly | 60 |\n",
    "| Daily | 24 x 60 |\n",
    "| Weekly | 24 x 60 x 7 |\n",
    "| Annual | 24 x 60 x 365 |\n",
    "\n",
    "> 빈도 설정을 위한 Python 함수 옵션: \n",
    "\n",
    "| Alias | Description |\n",
    "|--------|-----------------------|\n",
    "| B | Business day |\n",
    "| D | Calendar day |\n",
    "| W | Weekly |\n",
    "| M | Month end |\n",
    "| Q | Quarter end |\n",
    "| A | Year end |\n",
    "| BA | Business year end |\n",
    "| AS | Year start |\n",
    "| H | Hourly frequency |\n",
    "| T, min | Minutely frequency |\n",
    "| S | Secondly frequency |\n",
    "| L, ms | Millisecond frequency |\n",
    "| U, us | Microsecond frequency |\n",
    "| N, ns | Nanosecond frequency |\n",
    "\n",
    "| Method | Description |\n",
    "|---------|-----------------------------------------------------------|\n",
    "| bfill | Backward fill |\n",
    "| count | Count of values |\n",
    "| ffill | Forward fill |\n",
    "| first | First valid data value |\n",
    "| last | Last valid data value |\n",
    "| max | Maximum data value |\n",
    "| mean | Mean of values in time range |\n",
    "| median | Median of values in time range |\n",
    "| min | Minimum data value |\n",
    "| nunique | Number of unique values |\n",
    "| ohlc | Opening value, highest value, lowest value, closing value |\n",
    "| pad | Same as forward fill |\n",
    "| std | Standard deviation of values |\n",
    "| sum | Sum of values |\n",
    "| var | Variance of values |\n",
    "\n",
    "- **추세(Trend, $T_t$)**: 시계열이 시간에 따라 증가, 감소 또는 일정 수준을 유지하는 경우\n",
    "\n",
    "<center>\n",
    "<img src='Image/Trend_Increasing.png' width='400'>\n",
    "<img src='Image/Trend_Decreasing.png' width='400'>\n",
    "<img src='Image/Trend_None.png' width='400'></center>\n",
    "\n",
    "- **계절성(Seasonality, $S_t$)**: 일정한 빈도로 주기적으로 반복되는 패턴($m$)\n",
    "> 계설정 반영 방법큰 크게 2가지: 수치값 그대로, 발생 시점으로 분리  \n",
    "> 주기적 패턴이 12개월마다 반복($m$ = 12)  \n",
    "<center><img src='Image/Seasonal.png' width='400'></center>\n",
    "\n",
    "- **주기(Cycle, $C_t$)**: 일정하지 않은 빈도로 발생하는 패턴(계절성)  \n",
    "> 빈도가 1인 경우에도 발생 가능($m$ = 1).  \n",
    "<center><img src='Image/Cycle.png' width='400'></center>\n",
    "\n",
    "- **시계열 분해(추세/계절성/잔차(Residual, $e_t$))**:\n",
    "<center><img src='Image/Decomposed-into-its-trend-seasonal-and-irregular.png' width='600'></center>\n",
    "\n",
    "- **더미변수(Dummy Variables, $D_i$)**: 이진수(0 또는 1)의 형태로 변수를 생성하는 것으로 휴일, 이벤트, 캠페인, Outlier 등을 생성 가능\n",
    "    - **생성법:**\n",
    "        1. 범주형 변수(Categorical Variable)의 기준값을 미리 결정 (ex. 계절일 경우 봄)\n",
    "        2. 기준값을 제외한 채 더미변수를 생성 (ex. $D_1$ = 여름, $D_2$ = 가을, $D_3$ = 겨울) \n",
    "        2. 각 더미변수의 값을 0 또는 1로 채우며 1은 각 더미변수의 정의와 같음을 의미\n",
    "        \n",
    "> 확실한 패턴이 존재하는 경우에만 효과가 있으며 오히려 모델의 오류를 증가시킬 수 있음 \n",
    "\n",
    "<center><img src='Image/Dummy-variable-regression.jpg' width='400'></center>\n",
    "\n",
    "- **지연값(Lagged values, $Lag_t(X_1)$)**: 변수의 지연된 값을 독립변수로 반영하는 것으로,ARIMA/VAR/NNAR 등이 활용  \n",
    "<center><img src='Image/Lag-explanation.PNG' width='400'></center>\n",
    "\n",
    "- **시간변수**: 시간변수를 미시/거시 적으로 분리하거나 통합하여 생성된 변수  \n",
    "\n",
    "- **요약**:\n",
    "    >- <U>시계열 구성요소는 각 변수의 시간패턴을 파악하는데 중요\n",
    "    >- <U>FE를 통해 생성된 변수의 입력(Input) 형태로 모형 선택을 하는데 필요 \n",
    "    >- <U>생성된 변수의 패턴이 기존 모델에서 반영하지 않던 패턴이라면 예측 성능을 높임 \n",
    "    >- <U>예측성능 향상 뿐 아니라 결과를 해석하고 해당 속성을 분석하며 가능한 원인 식별에 도움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 알고리즘의 2가지 차별화 방향\n",
    ">**\"학습된 도메인 영역 내의 패턴 뿐 아니라 외부 시점의로 데이터를 확장 할 수 있어야 시계열 알고리즘\"**\n",
    "- 시계열 데이터나 FE를 통해 생성된 변수들은 미래시점을 생성시킬 수 있음  \n",
    "- 대다수의 기계학습 알고리즘은 학습된 도메인 영역에서의 패턴만을 추출\n",
    "\n",
    ">**\"시계열 알고리즘은 점추정이 아닌 구간추정 알고리즘으로 설명력 효과에 뿌리를 둠\"**\n",
    "- 대부분의 기계학습 모델은 통계분포에 기반하지 않기 때문에 점추정 알고리즘  \n",
    "- 신뢰구간의 정확성은 확신 할 수 없지만 점추정 보다 다양한 해석을 가능하게 함\n",
    "\n",
    ">**\"정확성 vs. 설명력 반비례 관례 존재\"**\n",
    "\n",
    "> - 기계학습 알고리즘에 대해,\n",
    "<center><img src='Image/Performance_Explanability.png' width='600'></center>\n",
    "\n",
    "> - 시계열 알고리즘에 대해,\n",
    "\n",
    "- **Dynamic Linear Model:** \n",
    "    - Bayesian-based Models\n",
    "    - [Generalized Autoregressive Conditional Heteroskedasticity(GARCH)](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity)\n",
    "    - [Vector Autoregression(VAR)](https://en.wikipedia.org/wiki/Vector_autoregression)\n",
    "- **Nueral Network Model:**\n",
    "    - Neural Networks Autoregression(NNAR)\n",
    "    - Recurrent Neural Network(RNN)\n",
    "    - Long Short-Term Memory(LSTM)\n",
    "    - Gated Recurrent Unit(GRU)\n",
    "\n",
    "<center><img src='Image/Performance_Explanability_TimeSeries.png' width='600'></center>\n",
    "\n",
    "- **설명력 최근 연구동향:**\n",
    "    - [LIME](https://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)\n",
    "    - [DARPA](https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증지표(Evaluation Metrics)과 잔차진단(Residuals Diagnostics)이 사용됨\n",
    ">**\"예측 분석 이후 예측이 잘 되었는지 그리고 데이터의 시간패턴이 잘 추출 되었는지 평가하는 것이 중요합니다.\"**\n",
    "- 검증지표는 예측값과 실제값이 얼마나 비슷한지를 측정하는 것이며, 모형이 시간특성을 잘 잡아내는지를 측정하지는 않음  \n",
    "- 시간특성 패턴이 잘 추출되었는지 확인하기 위해선 잔차(또는 에러) 진단을 통해 백색잡음(White Noise)와 얼마나 유사한지 측정=> <U>\"Residual Diagnostics\" or \"Error Analysis\"</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증지표(Evaluation Metrics)\n",
    "\n",
    "- **대표적 검증지표:**\n",
    "<center><img src='Image/Evaluation_Metric1.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric2.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric3.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric4.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric5.jpg' width='250'></center>\n",
    "\n",
    "| Acroynm | Full Name | Residual Operation? | Robust To Outliers? |\n",
    "|---------|--------------------------------|---------------------|---------------------|\n",
    "| MAE | Mean Absolute Error | Absolute Value | Yes |\n",
    "| MSE | Mean Squared Error | Square | No |\n",
    "| RMSE | Root Mean Squared Error | Square | No |\n",
    "| MAPE | Mean Absolute Percentage Error | Absolute Value | Yes |\n",
    "| MPE | Mean Percentage Error | N/A | Yes |\n",
    "    \n",
    "- **활용예시:** [Example of Modeling](https://pkg.robjhyndman.com/forecast/reference/accuracy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차진단(Residual Diagnostics)\n",
    "\n",
    ">**\"백색잡음(White Noise)는 2가지의 속성을 만족해야 하며 하나라도 만족하지 못하면 모델이 개선의 여지가 있음을 의미합니다.\"**  \n",
    "\n",
    "- **백색잡음(White Noise):**  \n",
    "<center><img src='Image/White_Noise.png' width='300'></center>\n",
    "    \n",
    "    - 잔차들은 정규분포이고, (unbiased) 평균 0과 일정한 분산을 가져야 함\n",
    "    <center>{$e_t$ : $t$ = $\\dots$, -2, -1, 0, 1, 2, $\\dots$} ~ $N(0,\\sigma^2_{e_t})$</center> \n",
    "    <center>$where$</center> \n",
    "    <center>$e_t$ = $Y_t$ - $\\hat{Y_t}$, $E(e_t)$ = 0, $Var(e_t)$ = $\\sigma^2_{e_t}$</center> \n",
    "    <center>$Cov(e_s, e_k)$ = 0 for different times ($s \\ne k$)</center>  \n",
    "    \n",
    "    - 잔차들이 시간의 흐름에 따라 상관성이 없어야 함:  \n",
    "        - 자기상관함수(Autocorrelation Fundtion([ACF](https://en.wikipedia.org/wiki/Autocorrelation)))를 통해 $Autocorrelation~=~0$인지 확인\n",
    "            - 공분산(Covariance): \n",
    "            <center>$Cov(Y_s, Y_k)$ = $E[(Y_s-E(Y_s))$$(Y_k-E(Y_k))]$ = $\\gamma_{s,k}$</center>\n",
    "            - 자기상관함수(Autocorrelation Function): \n",
    "            <center>$Corr(Y_s, Y_k)$ = $\\dfrac{Cov(Y_s, Y_k)}{\\sqrt{Var(Y_s)Var(Y_k)}}$ = $\\dfrac{\\gamma_{s,k}}{\\sqrt{\\gamma_s \\gamma_k}}$</center>\n",
    "            - 편자기상관함수(Partial Autocorrelation Function): $s$와 $k$사이의 상관성을 제거한 자기상관함수\n",
    "            <center>$Corr[(Y_s-\\hat{Y}_s, Y_{s-t}-\\hat{Y}_{s-t})]$ for $1<t<k$</center>\n",
    "            \n",
    "            \n",
    "- **회귀분석 가정과의 비교:**\n",
    "    - 종속변수와 독립변수 간에 선형성의 관계를 가져야 함\n",
    "    - 독립변수들 간에 서로 독립이어야 함\n",
    "    - 잔차의 분포가 정규분포이어야 함\n",
    "    - 잔차들의 분산이 서로 같아야 함\n",
    "    - 잔차들이 서로 독립적으로 움직여야 함\n",
    "\n",
    "\n",
    "- **자기상관 테스트 활용예시:**\n",
    "    - Apply a portmanteau test to check the hypothesis that residuals are uncorrelated.\n",
    "    <center><img src='Image/Portmanteau_Test.jpg' width='600'></center>\n",
    "    - Plot the Autocorrelation function (ACF) and evaluate that at least 95% of the spikes are on the interval.\n",
    "    <center><img src='Image/Residual_Plot.png' width='700'></center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열이 분석효과에 도움 될 시간영역(해상도)을 선택해야 함\n",
    "> **예측 정확성이 높은 시간영역을 선택하는 것이 좋습니다.**\n",
    "<center><img src='Image/Reduce_Error.png' width='400'></center>\n",
    "\n",
    "- **활용예시:**\n",
    "    - **분석목적:** 연간단위 비즈니스 목표를 예측\n",
    "        - Aim for the most granular level possible.  \n",
    "        - 일반적으로 월별 또는 분기별 데이터를 사용하면 연간 데이터보다 나은 예측이 가능할 것\n",
    "        - 월/분기별 예측치를 연간으로 환산시 오류가 늘어날 것 같지만 실제로는 반대의 경우가 많음  \n",
    "        - 만약 너무 세분화된 시간영역을 사용할 시 오류가 증가될 수 있음\n",
    "            - 연간 비즈니스 목표를 예측하는데 일별/시간별/분별/이하단위의 데이터를 사용하면 도움이 될까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 데이터/분석은 높은 정확도를 낳거나 높은 에러를 발생시킴\n",
    "- **높은정확도(High Accuracy):** 과거 패턴이 미래에도 그대로 유지가 된다면 예측 정확도가 높아짐  \n",
    "- **높은에러(High Error):** 패턴이 점차적으로 또는 갑자기 변경되면 예측값은 실제값에서 크게 벗어날 수 있음  \n",
    "    - **Black Swan:** <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    "    - **White Swan:** <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    "    - **Gray Swan:** <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 데이터 관리는 장/단점 존재\n",
    "- 수천/수백만/수십억 데이터를 기계학습에 사용할 수 있지만 시계열로 데이터를 정리하면 데이터 감소 발생가능\n",
    "- 모든 시간범위가 예측성능에 도움되지 않을 수 있기에 특정기간의 시간영역 분석만 필요할 수도 있음\n",
    "\n",
    "- **고성능 시계열 Database 필요:**\n",
    "> [Time Series Database(TSDB)](http://shop.oreilly.com/product/0636920035435.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비 의사결정\n",
    "\n",
    "> **일반적준비(Simple Validation):**\n",
    "- **훈련셋(Training set):** 이름처럼 일반적으로 전체 데이터의 60%를 사용하여 기계학습을 하는데 사용됨  \n",
    "- **검증셋(Validation set):** \n",
    "    - 개발셋이라고도 하며, 일반적으로 전체 데이터의 20%를 사용함\n",
    "    - 훈련된 여러가지 모델들의 성능을 테스트 하는데 사용되며 모델 선택의 기준이 됨\n",
    "- **테스트셋(Testing set):** 전체 데이터의 20%를 사용하며 최종 모델의 정확성을 확인하는 목적에 사용됨\n",
    "\n",
    "<center><img src='Image/DataSplit_Simple.png' width='500'></center>\n",
    "\n",
    "> **$K$교차검사($K$-fold Cross Validation):**  \n",
    "1. 훈련셋을 복원없이 $K$개로 분리한 후, $K-1$는 하위훈련셋으로 나머지 1개는 검증셋으로 사용함  \n",
    "2. 검증셋과 하위훈련셋을 번갈아가면서 $K$번 반복하여 각 모델별로 $K$개의 성능 추정치를 계산  \n",
    "3. $K$개의 성능 추정치 평균을 최종 모델 성능 기준으로 사용  \n",
    "\n",
    "<center><img src='Image/DataSplit_Kfold.png' width='500'></center>\n",
    "\n",
    "> **간단한준비(Holdout Validation):**\n",
    "- **훈련셋(Training set):** 일반적으로 전체 데이터의 70% 사용 \n",
    "- **테스트셋(Testing set):** 일반적으로 전체 데이터의 30% 사용\n",
    "\n",
    "> **$K$-fold vs. Random-subsamples vs. Leave-one-out vs. Leave-$p$-out**  \n",
    ">- **$K$-fold**\n",
    "<center><img src='Image/DataSplit_ver1.png' width='500'></center>\n",
    "\n",
    ">- **Random-subsamples**\n",
    "<center><img src='Image/DataSplit_ver2.png' width='500'></center>\n",
    "\n",
    ">- **Leave-one-out**\n",
    "<center><img src='Image/DataSplit_ver3.png' width='500'></center>\n",
    "\n",
    ">- **Leave-$p$-out**\n",
    "<center><img src='Image/DataSplit_ver4.png' width='500'></center>\n",
    "\n",
    "> **시계열준비(Time Series Validation):**\n",
    ">- 시계열 데이터인 경우 랜덤성(set.seed)을 부여하면 안되고 시간축 유지가 핵심!\n",
    "    - **훈련셋(Training set):** 가장 오래된 데이터\n",
    "    - **검증셋(Validation set):** 그 다음 최근 데이터\n",
    "    - **테스트셋(Testing set):** 가장 최신의 데이터\n",
    "<center><img src='Image/DataSplit_TimeSeries.png' width='500'></center>\n",
    "\n",
    ">- **1스텝 교차검사(One-step Ahead Cross-validation)**\n",
    "<center><img src='Image/DataSplit_TimeSeries_ver1.png' width='500'></center>\n",
    "\n",
    ">- **2스텝 교차검사(Two-step Ahead Cross-validation)**\n",
    "<center><img src='Image/DataSplit_TimeSeries_ver2.png' width='500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능비교 의사결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting vs Overfitting\n",
    "- **과소적합(Underfitting):** 단순모델 또는 높은편향(충분하지 않은 데이터패턴 학습)\n",
    "- **과적합(Overfitting):** 복잡한모델 또는 높은분산(주어진 데이터패턴에만 효과)\n",
    "\n",
    "<center><img src='Image/Underfitting_Overfitting.jpg' width='600'></center>\n",
    "<center><img src='Image/Underfitting_Overfitting.png' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편향-분산 상충관계(Bias-variance Trade-off)  \n",
    "- **편향(Bias):** 예측값과 실제값의 차이로 모델의 방향,\n",
    "- **편향(Bias(Real)):** <U>복잡한 현실의 단순화로 미처 반영하지 못한 복잡성 => 작다면 Training의 패턴(복잡성) 최대반영</U>\n",
    "- **분산(Variance):** 예측값이 평균으로부터 퍼진 정도, \n",
    "- **분산(Variance(Real)):** <U>다른 Training을 사용했을때 발생할 변화 => 크다면 Training에 따라 많은변화 발생가능</U>\n",
    "\n",
    "<center><img src='Image/Bias_Variance1.jpeg' width='400'></center>\n",
    "<center><img src='Image/Bias_Variance2.png' width='280'></center>\n",
    "<center><img src='Image/Bias_Variance3.png' width='500'></center>\n",
    "<center><img src='Image/Bias_Variance4.png' width='400'></center>\n",
    "\n",
    "- **높은편향과 높은분산 문제를 최소화하는 방법은?**\n",
    "<center><img src='Image/Bias_Variance_Reduce.png' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 검토사항 의사결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중공선성 제거\n",
    "$Y$ = $\\beta_0$ + $\\beta_1$$X_1$ + $\\beta_2$$X_2$ + ... + $\\beta_k$$X_k$ + $\\epsilon$  \n",
    "\n",
    "$VIF_i$ = $Var(\\hat{\\beta}_i)$ = $\\dfrac{\\sigma^2_{\\epsilon}}{(n-1)Var(X_i)}$ $\\cdot$ $\\dfrac{1}{1-R^2_i}$\n",
    "\n",
    "$where$\n",
    "\n",
    "$R^2_i$: $X_i$ 를 다른 변수로 회귀분석 한 후의 성능지표\n",
    "\n",
    "=> 다른 변수의 의존성이 높을수록 $R^2_i$가 커지고 $VIF_i$도 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분석마무리 의사결정\n",
    "> **\"모델링이 데이터의 패턴을 최대한 반영했을 경우 분석을 마무리 해도 좋다\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://nowenlightenme.com/2018/03/18/types-of-machine-learning/  \n",
    "https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861  \n",
    "https://nowenlightenme.com/2018/03/18/types-of-machine-learning/  \n",
    "https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/  \n",
    "http://benalexkeen.com/resampling-time-series-data-with-pandas/  \n",
    "https://www.researchgate.net/figure/The-original-time-series-decomposed-into-its-trend-seasonal-and-irregular-ie_fig2_279249485  \n",
    "https://towardsdatascience.com/interpretability-vs-accuracy-the-friction-that-defines-deep-learning-dae16c84db5c  \n",
    "https://www.datascience.com/blog/time-series-forecasting-machine-learning-differences  \n",
    "https://www.dataquest.io/blog/understanding-regression-error-metrics/  \n",
    "http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/08_CV_Ensembling.html  \n",
    "https://www.quora.com/For-K-fold-cross-validation-what-k-should-be-selected  \n",
    "http://www.ebc.cat/2017/01/31/cross-validation-strategies/  \n",
    "https://uc-r.github.io/ts_benchmarking  \n",
    "https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76  \n",
    "https://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "531px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
