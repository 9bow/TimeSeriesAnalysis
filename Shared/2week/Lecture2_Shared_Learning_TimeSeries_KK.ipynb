{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/cheonbi/DataScience/blob/master/Image/2week.png?raw=true' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Style and Algorithms\n",
    "\n",
    "## Learning Style\n",
    "There are 3 or 4 different ways an algorithm can model a problem.  \n",
    "This taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.  \n",
    "(Supervised vs Unsupervised vs Semi-supervised vs Reinforcement Learning)  \n",
    "\n",
    "- Overall types of ML (3 Types):  \n",
    "<img src='https://github.com/cheonbi/DataScience/blob/master/Image/ML_Type_Application_Circle.jpg?raw=true' width='600'>\n",
    "\n",
    "- With other examples (4 Types):\n",
    "<img src='https://github.com/cheonbi/DataScience/blob/master/Image/ML_Type_Application.png?raw=true' width='600'>\n",
    "\n",
    "- With other examples (Roles):  \n",
    "<img src='https://github.com/cheonbi/DataScience/blob/master/Image/ML_Type_Glance.jpg?raw=true' width='600'>\n",
    "\n",
    "|               \t| Supervised Learning \t| Unsupervised Learning \t| Semi-supervised Learning \t|\n",
    "|---------------\t|---------------------\t|-----------------------\t|--------------------------\t|\n",
    "| Input Data    \t| labeled             \t| unlabeled             \t| labeled + unlabeled      \t|\n",
    "| Output Result \t| labeled             \t| unlabeled             \t| labeled + unlabeled      \t|\n",
    "| |<img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Supervised-Learning-Algorithms.png' width='150'>|<img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Unsupervised-Learning-Algorithms.png' width='150'>|<img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Semi-supervised-Learning-Algorithms.png' width='150'>|\n",
    "\n",
    "## Algorithms by Learning Style [(Summary)](https://en.wikipedia.org/wiki/Outline_of_machine_learning)\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "| Regression Algorithms | Instance-based Algorithms | Regularization Algorithms | Decision Tree Algorithms | Bayesian Algorithms | Artificial Neural Network Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Ordinary Least Squares Regression (OLSR) | k-Nearest Neighbor (kNN) | Ridge Regression | Classification and Regression Tree (CART) | Naive Bayes | Perceptron |\n",
    "| Linear Regression | Learning Vector Quantization (LVQ) | Least Absolute Shrinkage and Selection Operator (LASSO) | Iterative Dichotomiser 3 (ID3) | Gaussian Naive Bayes | Back-Propagation |\n",
    "| Logistic Regression | Self-Organizing Map (SOM) | Elastic Net | C4.5 and C5.0 (different versions of a powerful approach) | Multinomial Naive Bayes | Hopfield Network |\n",
    "| Stepwise Regression | Locally Weighted Learning (LWL) | Least-Angle Regression (LARS) | Chi-squared Automatic Interaction Detection (CHAID) | Averaged One-Dependence Estimators (AODE) | Radial Basis Function Network (RBFN) |\n",
    "| Multivariate Adaptive Regression Splines (MARS) |  |  | Decision Stump | Bayesian Belief Network (BBN) |  |\n",
    "| Locally Estimated Scatterplot Smoothing (LOESS) |  |  | M5 | Bayesian Network (BN) |  |\n",
    "|  |  |  | Conditional Decision Trees |  |  |\n",
    "| <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Regression-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Instance-based-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Regularization-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Decision-Tree-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Bayesian-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Artificial-Neural-Network-Algorithms.png' width='150'> |\n",
    "\n",
    "### Unsupervised Learning  \n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) |  | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering |  | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "|  |  | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) |  |\n",
    "|  |  | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) |  |\n",
    "|  |  | Linear Discriminant Analysis (LDA) | Random Forest |  |\n",
    "|  |  | Mixture Discriminant Analysis (MDA) |  |  |\n",
    "|  |  | Quadratic Discriminant Analysis (QDA) |  |  |\n",
    "|  |  | Flexible Discriminant Analysis (FDA) |  |  |\n",
    "| <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Clustering-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Ensemble-Algorithms.png' width='150'> | <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/11/Deep-Learning-Algorithms.png' width='150'> |\n",
    "<center>(https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis differs from Machine Learning\n",
    "There is a radical change in moving from machine learning to time-series forecasting.  \n",
    "The objective of a predictive model is to estimate the value of an unknown variable. A time series has time ($t$) as an independent variable (in any unit you can think of) and a target dependent variable ($y_t$). The output of the model is the predicted value for $y$ at time $t$, ie. ($\\hat{y_t}$).  \n",
    "\n",
    "Whenever data is recorded at regular intervals of time, it is called a time series. You can think of this type of variable in two ways:  \n",
    "1) The data is univariate, but it has an index (time) that creates an implicit order.  \n",
    "2) The dataset has two dimensions: the time (independent variable) and the variable itself as dependent variable.  \n",
    "\n",
    "If you have experience working in machine learning, you must make some adjustments when working with time series.\n",
    "\n",
    "## Time-series Features should be handled with Care (Time Series Components)\n",
    "As a data scientist, you may already be used to creating features, either manually (feature engineering) or automatically (feature learning). Either way, creating features is one of the most important and time-consuming tasks in analysis.  \n",
    "\n",
    "However, in time series forecasting, you don’t create features — at least not in the traditional sense. This is especially true when you want to forecast several steps ahead, and not just the following value.  \n",
    "\n",
    "This does not mean that features are completely off limits. Instead, they should be used with care because of the following reasons:  \n",
    "1) It is not clear what the future real values will be for those features.  \n",
    "2) If the features are predictable, i.e., they have some patterns, you can build a forecast model for each of them. However, keep in mind that using predicted values as features will propagate the error to the target variable, which may cause higher errors or produce biased forecasts.  \n",
    "3) A pure time series model may have similar or even better performance than one using features.  \n",
    "\n",
    "Besides, some forecasting models are only based on historical values of the variable, like Exponential Smoothing (ETS) and Autoregressive Integrated Moving Average (ARIMA) models.  \n",
    "\n",
    "Is it also possible to combine time series with feature engineering using time series components and time-based features. The first refers to the properties (components) of a time series, and the latter refers to time-related features, which have definite patterns and can be calculated in a deterministic way. You can add them to any time-series models that can handle predictors.  \n",
    "\n",
    "- **Frequency**: The number of observations before the seasonal pattern repeats. You need to decide which of these is the most important.  \n",
    "> For the yearly seasonal pattern,  \n",
    "    \n",
    "| Data | frequency |\n",
    "|-----------|-----------|\n",
    "| Annual | 1 |\n",
    "| Quarterly | 4 |\n",
    "| Monthly | 12 |\n",
    "| Weekly | 52 |\n",
    "\n",
    "> For the data with daily observations,  \n",
    "   \n",
    "| Seasonality | frequency |\n",
    "|-------------|-----------|\n",
    "| Weekly | 7 |\n",
    "| Annual | 365 |\n",
    "\n",
    "> Fot the data with minute observations,  \n",
    "    \n",
    "| Seasonality | frequency |\n",
    "|-------------|-----------|\n",
    "| Hourly | 60 |\n",
    "| Daily | 24 x 60 |\n",
    "| Weekly | 24 x 60 x 7 |\n",
    "| Annual | 24 x 60 x 365 |\n",
    "\n",
    "> Frequency and methods for resampling options: \n",
    "\n",
    "| Alias | Description |\n",
    "|--------|-----------------------|\n",
    "| B | Business day |\n",
    "| D | Calendar day |\n",
    "| W | Weekly |\n",
    "| M | Month end |\n",
    "| Q | Quarter end |\n",
    "| A | Year end |\n",
    "| BA | Business year end |\n",
    "| AS | Year start |\n",
    "| H | Hourly frequency |\n",
    "| T, min | Minutely frequency |\n",
    "| S | Secondly frequency |\n",
    "| L, ms | Millisecond frequency |\n",
    "| U, us | Microsecond frequency |\n",
    "| N, ns | Nanosecond frequency |\n",
    "\n",
    "| Method | Description |\n",
    "|---------|-----------------------------------------------------------|\n",
    "| bfill | Backward fill |\n",
    "| count | Count of values |\n",
    "| ffill | Forward fill |\n",
    "| first | First valid data value |\n",
    "| last | Last valid data value |\n",
    "| max | Maximum data value |\n",
    "| mean | Mean of values in time range |\n",
    "| median | Median of values in time range |\n",
    "| min | Minimum data value |\n",
    "| nunique | Number of unique values |\n",
    "| ohlc | Opening value, highest value, lowest value, closing value |\n",
    "| pad | Same as forward fill |\n",
    "| std | Standard deviation of values |\n",
    "| sum | Sum of values |\n",
    "| var | Variance of values |\n",
    "\n",
    "- **Trend**: A trend exists when a series increases, decreases, or remains at a constant level with respect to time. <U>Therefore, the time is taken as a feature.</U>   \n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/trend_increasing.png?width=1912&name=trend_increasing.png' width='400'>\n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/trend_decreasing.png?width=1912&name=trend_decreasing.png' width='400'>\n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/trend_none.png?width=1912&name=trend_none.png' width='400'>  \n",
    "\n",
    "- **Seasonality**: This refers to the property of a time series that displays periodical patterns that repeats at a constant frequency ($m$).  \n",
    "> In the following example, you can observe a seasonal component with $m$ = 12, which means that the periodical pattern repeats every twelve months. (Usually, to handle seasonality, time series models include seasonal variables as dummy features, using $m$—1 binary variables to avoid correlation between features.)\n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/seasonal.png?width=1912&name=seasonal.png' width='400'>  \n",
    "\n",
    "- **Cycle**: Cycles are seasons that do not occur at a fixed rate.  \n",
    "> These do not repeat at regular time intervals and may occur even if the frequency is 1 (m = 1).  \n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/lynx.png?width=1912&name=lynx.png' width='400'>  \n",
    "\n",
    "- **Decomposition of Series**:\n",
    "<img src='https://www.researchgate.net/profile/Louis_Tay/publication/279249485/figure/fig2/AS:272568091410451@1441996705460/The-original-time-series-decomposed-into-its-trend-seasonal-and-irregular-ie.png' width='600'>  \n",
    "\n",
    "- **Dummy variables**: Similar to how seasonality can be added as a binary feature, other features can be added in binary format to the model. You can add holidays, special events, marketing campaigns, whether a value is outlier or not, etc.  \n",
    "    - **How to:**\n",
    "        1. The criteria value of categorical variable is predetermined\n",
    "        2. Create a dummy variables except for the criteria value\n",
    "        2. The values of each dummy variable has a value of 0 or 1\n",
    "        \n",
    "> However, you should remember that these variables need to have definite patterns.  \n",
    "\n",
    "<img src='https://image.slidesharecdn.com/dummy-variable-regression-1836/95/dummy-variable-regression-7-728.jpg?cb=1229151573' width='400'>\n",
    "\n",
    "- **Lagged values**: You can include lagged values of the variable as predictors. Some models like ARIMA, Vector Autoregression (VAR), or Autoregressive Neural Networks (NNAR) work this way.  \n",
    "<img src='https://www.business-science.io/assets/lag-explanation.PNG' width='400'>\n",
    "\n",
    "- **Number of days**: These can be easily calculated even for future months/quarters and may affect forecasts, especially for financial data.  \n",
    "\n",
    "- **Summary**:\n",
    "    - <U>Time series components are highly important to analyzing the variable of interest in order to understand its behavior, what patterns it has.\n",
    "    - To be able to choose and fit an appropriate time-series model. \n",
    "    - Time series predictors may help some models to recognize additional patterns and improve the quality of forecasts. \n",
    "    - Both time series components and features are key to interpreting the behavior of the time series, analyzing its properties, identifying possible causes, and more.</U>  \n",
    "\n",
    "## A different Time-series Algorithmic approach is required\n",
    ">**\"The objective of time series is to project into the future.\"**\n",
    "- One of the most important properties an algorithm needs in order to be considered a time-series algorithm is the ability to extrapolate patterns outside of the domain of training data.\n",
    "- Many machine learning algorithms do not have this capability, as they tend to be restricted to a domain that is defined by training data.\n",
    "\n",
    ">**\"Another important property of a time series algorithm is the ability to derive confidence intervals.\"**\n",
    "- The most machine learning models do not have this ability because they are not all based on statistical distributions.\n",
    "- Confidence intervals can be estimated, but they may not be as accurate.\n",
    "\n",
    "- **Dynamic Linear Model:** \n",
    "    - Bayesian-based Models\n",
    "    - [Generalized Autoregressive Conditional Heteroskedasticity(GARCH)](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity)\n",
    "    - [Vector Autoregression(VAR)](https://en.wikipedia.org/wiki/Vector_autoregression)\n",
    "- **Nueral Network Model:**\n",
    "    - Neural Networks Autoregression(NNAR)\n",
    "    - Recurrent Neural Network(RNN)\n",
    "    - Long Short-Term Memory(LSTM)\n",
    "    - Gated Recurrent Unit(GRU)\n",
    "<img src='https://www.datascience.com/hs-fs/hubfs/Screen%20Shot%202018-05-22%20at%201.04.57%20PM.png?width=1500&name=Screen%20Shot%202018-05-22%20at%201.04.57%20PM.png' width='600'>\n",
    "> **Assumptions of Linear Regression:**\n",
    "    > - Linearilty of X and Y\n",
    "    > - Independency of Xs\n",
    "    > - Homoscedasticity of Residuals\n",
    "    > - Normality of Residuals\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*Uh9xiFRHZn0150X2JXUufw.png' width='600'>\n",
    "\n",
    "- **Interpretability Research:**\n",
    "    - [LIME](https://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)\n",
    "    - [DARPA](https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)\n",
    "    \n",
    "## Both Evaluation Metrics and Residuals Diagnostics are used\n",
    ">**\"After a forecasting model has been fit, it is important to assess how well it is able to capture patterns\"**\n",
    "- While evaluation metrics help determine how close the fitted values are to the actual ones, they do not evaluate whether the model properly fits the time series.\n",
    "- As you are trying to capture the patterns of a time series, you would expect the errors to behave as white noise. => <U>\"Error Analysis\" or \"Residual Diagnostics\"</U>\n",
    "\n",
    "- **Example of metrics:**\n",
    "<img src='https://i.imgur.com/DT4H1Yk.jpg' width='300'>\n",
    "<img src='https://i.imgur.com/BmBC8VW.jpg' width='300'>\n",
    "<img src='https://i.imgur.com/vB3UAiH.jpg' width='300'>\n",
    "<img src='https://i.imgur.com/YYMpqUY.jpg' width='300'>\n",
    "<img src='https://i.imgur.com/ndIXERr.jpg' width='250'>\n",
    "\n",
    "<center>[Example of Modeling](https://pkg.robjhyndman.com/forecast/reference/accuracy.html)</center>\n",
    "\n",
    "| Acroynm | Full Name | Residual Operation? | Robust To Outliers? |\n",
    "|---------|--------------------------------|---------------------|---------------------|\n",
    "| MAE | Mean Absolute Error | Absolute Value | Yes |\n",
    "| MSE | Mean Squared Error | Square | No |\n",
    "| RMSE | Root Mean Squared Error | Square | No |\n",
    "| MAPE | Mean Absolute Percentage Error | Absolute Value | Yes |\n",
    "| MPE | Mean Percentage Error | N/A | Yes |\n",
    "\n",
    ">**\"White noise must have the following properties and if either of the two properties are not present, it means that there is room for improvement in the model\"**\n",
    "- The residuals are uncorrelated ($Autocorrelation~=~0$) => [ACF](https://en.wikipedia.org/wiki/Autocorrelation)\n",
    "- The residuals follow a normal distribution, with zero mean (unbiased) and constant variance => $e_t \\sim N(0,\\sigma^2)$\n",
    "\n",
    "- **Example of autocorrelation test:**\n",
    "    - Apply a portmanteau test to check the hypothesis that residuals are uncorrelated.\n",
    "    <img src='https://i.stack.imgur.com/2P722.jpg' width='600'>\n",
    "    - Plot the Autocorrelation function (ACF) and evaluate that at least 95% of the spikes are on the interval.\n",
    "    <img src='https://www.datascience.com/hs-fs/hubfs/interval.png?width=902&name=interval.png' width='700'>\n",
    "    \n",
    "## The right Resolution must be chosen\n",
    ">**\"Try to work at an appropriate level of resolution\"**\n",
    "- **Example:**\n",
    "    - **Objective:** Assume that the business objective is to forecast at a yearly level.\n",
    "        - Aim for the most granular level possible.  \n",
    "        - In general, using monthly or quarterly data may yield better results than a yearly forecast.  \n",
    "        - May think that after adding the forecasts the error may propagate to the total. However, it is the opposite case.  \n",
    "        <img src='https://www.datascience.com/hs-fs/hubfs/Screen%20Shot%202018-05-22%20at%201.59.24%20PM.png?width=928&name=Screen%20Shot%202018-05-22%20at%201.59.24%20PM.png' width='400'>\n",
    "        - Keep in mind that working at a level that is too granular may present noisy data that is difficult to model.\n",
    "            - In our example where we forecasted at a yearly level, using quarterly, monthly, or even a weekly level may be appropriate. But a daily, hourly, or a lower level may be too granular and noisy for the problem. \n",
    "            \n",
    "## Some Models will have either High Accuracy or High Error\n",
    "- **High Accuracy:** You are assuming that past patterns are indicators of what may occur in the future, and therefore they get replicated or projected. This means that if patterns continue the way they are, your forecasts will be highly accurate.\n",
    "- **High Error:** If patterns change, either gradually or abruptly, the forecasts may deviate highly from actual results. There is a chance that “black/white/gray swan” events may occur.\n",
    "    - **Black Swan:** <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    "    - **White Swan:** <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    "    - **Gray Swan:** <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>\n",
    "    \n",
    "## There may be smaller datasets.\n",
    "- **Drawback of Small Dataset?**: You may be used to feeding thousands, millions, or billions of data points into a machine learning model, but this is not always the case with time series. In fact, you may be working with small- to medium-sized time series, depending on the frequency and type of variable.  \n",
    "> [Time Series Database(TSDB)](http://shop.oreilly.com/product/0636920035435.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation of Time Series\n",
    "\n",
    "## Data Split  \n",
    "\n",
    "> **Simple Validation:**\n",
    "- **Training set:** Typically 60% of the data. As the name suggests, this is used for training a machine learning model.\n",
    "- **Validation set:** Also called the the development set. This is typically 20% of the data. This set is not used during training. It is used to test the quality of the trained model. Errors on the validation set are used to guide the choice of model. Even though this set is not used for training, the fact it was used for model selection makes it a bad choice for reporting the final accuracy of the model.\n",
    "- **Testing set:** Typically 20% of the data. Its only purpose is to report the accuracy of the final model.\n",
    "\n",
    "<img src='http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/fig-holdout.png' width='500'>\n",
    "\n",
    "> **$K$-fold Cross Validation:**\n",
    "- In  $K$ -fold cross-validation (CV), we randomly split the training dataset into  $K$  folds without replacement, where  $K$−1  folds are used for the model training and the remaining 1 fold is for validating. This procedure is repeated  $K$  times so that we obtain  $K$  models and  $K$  performance estimates. Then we take their average as the final performance estimate.\n",
    "\n",
    "<img src='https://qph.fs.quoracdn.net/main-qimg-29c6f21ce298acfa228f37448f844ab8' width='500'>\n",
    "\n",
    "> **Holdout Validation:**\n",
    "- **Training set:** Typically 70% of the data. \n",
    "- **Testing set:** Typically 30% of the data. \n",
    "\n",
    "> **$K$-fold vs. Random-subsamples vs. Leave-one-out vs. Leave-$p$-out**\n",
    "- **$K$-fold**\n",
    "<img src='http://www.ebc.cat/wp-content/uploads/2017/01/kfold.png' width='500'>\n",
    "- **Random-subsamples**\n",
    "<img src='http://www.ebc.cat/wp-content/uploads/2017/01/random_subsampling.png' width='500'>\n",
    "- **Leave-one-out**\n",
    "<img src='http://www.ebc.cat/wp-content/uploads/2017/01/leave_one_out.png' width='500'>\n",
    "- **Leave-$p$-out**\n",
    "<img src='http://www.ebc.cat/wp-content/uploads/2017/01/leave_p_out.png' width='500'>\n",
    "\n",
    "> **Time Series Validation:**\n",
    "- 시계열 데이터인 경우 랜덤성(set.seed)을 부여하면 안되고 시간축 유지가 핵심!\n",
    "    - **Training set:** 가장 오래된 데이터\n",
    "    - **Validation set:** 그 다음 최근 데이터\n",
    "    - **Testing set:** 가장 최신의 데이터\n",
    "<img src='https://uc-r.github.io/public/images/analytics/time_series/partitioning-1.png' width='500'>\n",
    "- **One-step Ahead Cross-validation**\n",
    "<img src='https://uc-r.github.io/public/images/analytics/time_series/ts_validation.png' width='500'>\n",
    "- **Two-step Ahead Cross-validation**\n",
    "<img src='https://uc-r.github.io/public/images/analytics/time_series/two_step_cv.png' width='500'>\n",
    "\n",
    "## Underfitting vs Overfitting\n",
    "- **Underfitting:** simple model or higher bias(not sufficient data pattern)\n",
    "- **Overfitting:** complex model or higher variance(suit for only given data pattern)\n",
    "<img src='https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg' width='600'>\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*_7OPgojau8hkiPUiHoGK_w.png' width='600'>\n",
    "\n",
    "## Bias-variance Trade-off  \n",
    "- **Bias:** 예측값과 실제값의 차이로 모델의 방향,\n",
    "- **Bias(Real):**<U>복잡한 현실의 단순화로 미처 반영하지 못한 복잡성 => 작다면 Training의 패턴(복잡성) 최대반영</U>\n",
    "- **Variance:** 예측값이 평균으로부터 퍼진 정도, \n",
    "- **Variance(Real):**<U>다른 Training을 사용했을때 발생할 변화 => 크다면 Training에 따라 많은변화 발생가능</U>\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*v63L_h5WXGOb4o6oh_daAA.jpeg' width='400'>\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*BtpFTBrGaQNE3TvU-0EVSQ.png' width='280'>\n",
    "<img src='https://cdn-images-1.medium.com/max/800/1*e7VaoBh5apjaM2p4afkFyg.png' width='500'>\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/1*buEoljozRp-i5-rIZm4RWg.png' width='400'>\n",
    "- **How to fix a high bias or a high variance problem?**\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/02/Machine-Learning-Workflow.png' width='600'>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "527px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
