{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습(Machine Learning) 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화 회귀분석(Regularized Method, Penalized Method, Contrained Least Squares)\n",
    "\n",
    "> **\"선형회귀 계수(Weight)에 대한 제약 조건을 추가함으로써 모형이 과도하게 최적화되는 현상, 즉 과최적화를 막는 방법\"**  \n",
    "> **\"과최적화는 계수 크기를 과도하게 증가하는 경향이 있기에, 정규화 방법에서의 제약 조건은 일반적으로 계수의 크기를 제한하는 방법\"**  \n",
    "\n",
    "**0) Standard Regression:** \n",
    "\n",
    "<center>\n",
    "$\\hat{\\beta} = arg\\underset{\\hat{\\beta}}{min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k \\beta_i x_{ij}\\Bigr)^2\\Biggr]$\n",
    "</center>\n",
    "\n",
    "\n",
    "**1) Ridge Regression:**  \n",
    "\n",
    "<center>\n",
    "$\\hat{\\beta} = arg\\underset{\\hat{\\beta}}{min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k \\beta_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k \\beta_i^2\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)$\n",
    "</center>\n",
    "\n",
    "\n",
    "**2) Lasso(Least Absolute Shrinkage and Selection Operator) Regression:**  \n",
    "\n",
    "<center>\n",
    "$\\hat{\\beta} = arg\\underset{\\hat{\\beta}}{min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k \\beta_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k \\left|\\beta_i \\right|\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)$\n",
    "</center>\n",
    "\n",
    "<center><img src='Image/Ridge_Lasso.png' width='600'></center> \n",
    "\n",
    "\n",
    "**3) Elastic Net:**  \n",
    "\n",
    "<center>\n",
    "$\\hat{\\beta} = arg\\underset{\\hat{\\beta}}{min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k \\beta_i x_{ij}\\Bigr)^2 + \\lambda_1 \\displaystyle \\sum_{i=0}^k \\left|\\beta_i \\right| + \\lambda_2 \\displaystyle \\sum_{i=0}^k \\beta_i^2\\Biggr] \\\\ where~\\lambda_1~and~\\lambda_2~are~hyper~parameters(given~by~human)$\n",
    "</center>\n",
    "\n",
    "\n",
    "**4) Summary**  \n",
    "\n",
    "> - **Standard:** \n",
    "    <center><img src='Image/Regression_Result_Standard.png' width='400'></center>  \n",
    "> - **Ridge:** \n",
    "    - 알고리즘이 모든 변수들을 포함하려 하기 때문에 계수의 크기가 작아지고 모형의 복잡도가 줄어듬  \n",
    "    - 모든 변수들을 포함하려 하므로 변수의 수가 많은 경우 효과가 좋지 않으나 과적합(Overfitting)을 방지하는데 효과적  \n",
    "    - 다중공선성이 존재할 경우, 변수 간 상관관계에 따라 계수로 다중공선성이 분산되기에 효과가 높음  \n",
    "    <center><img src='Image/Regression_Result_Ridge1.png' width='400'><img src='Image/Regression_Result_Ridge2.png' width='400'></center>   \n",
    "> - **LASSO:**  \n",
    "    - 알고리즘이 최소한의 변수를 포함하여 하기 때문의 나머지 변수들의 계수는 0이됨 (Feature Selection 기능)  \n",
    "    - 변수선택 기능이 있기에 일반적으로 많이 사용되는 이점이 있지만 특정변수에 대한 계수가 커지는 단점 존재  \n",
    "    - 다중공선성이 존재할 경우, 특정 변수만을 선택하는 방식이라 **Ridge**에 비해 다중공선성 문제에 효과가 낮음  \n",
    "    <center><img src='Image/Regression_Result_Lasso1.png' width='400'><img src='Image/Regression_Result_Lasso2.png' width='400'></center>\n",
    "> - **Elastic Net:**  \n",
    "    - 큰 데이터셋에서 Ridge와 LASSO의 효과를 모두 반영하기에 효과가 좋음 (적은 데이터셋은 효과 낮음)  \n",
    "    <center><img src='Image/Regression_Result_EN.png' width='400'></center>  \n",
    "    \n",
    "- **파라미터 세팅**  \n",
    "    > [**1) \"statsmodels\"**](https://datascienceschool.net/view-notebook/83d5e4fff7d64cb2aecfd7e42e1ece5e/)\n",
    "    - **Ridge:** \n",
    "    <center>\n",
    "    $\\lambda_1 = 0,~~0 < \\lambda_2 < 1 \\\\ => L_1 = 0,~~alpha \\ne 0$\n",
    "    </center>\n",
    "    - **LASSO:** \n",
    "    <center>\n",
    "    $0 < \\lambda_1 < 1,~~\\lambda_2 = 0 \\\\ => L_1 = 1,~~alpha \\ne 0$\n",
    "    </center>\n",
    "    - **Elastic Net:** \n",
    "    <center>\n",
    "    $0 < (\\lambda_1, \\lambda_2) < 1 \\\\ => 0 < L_1 < 1,~~alpha \\ne 0$\n",
    "    </center>\n",
    "    \n",
    "    > **2) \"sklearn\"**\n",
    "    - [**Ridge:**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "    <center>\n",
    "    $0 < (\\lambda = alpha) < 1$\n",
    "    </center>\n",
    "    - [**LASSO:**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "    <center>\n",
    "    $0 < (\\lambda = alpha) < 1$\n",
    "    </center>\n",
    "    - [**Elastic Net:**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "    <center>\n",
    "    $0 < (\\lambda_1, \\lambda_2) < 1 \\\\ => 0 < L_1 < 1,~~alpha \\ne 0$\n",
    "    </center>\n",
    "    \n",
    "~~~\n",
    "# Ridge\n",
    "fit = Ridge(alpha=0.5, fit_intercept=True, normalize=True, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# LASSO\n",
    "fit = Lasso(alpha=0.5, fit_intercept=True, normalize=True, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# Elastic Net\n",
    "fit = ElasticNet(alpha=0.01, l1_ratio=1, fit_intercept=True, normalize=True, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀분석 알고리즘 정리\n",
    "\n",
    "- **변수 세팅에 따른 분류:**\n",
    "\n",
    "<center><img src='Image/Regression-Algorithms-Tree1.png' width='800'></center> \n",
    "\n",
    "- **문제 해결에 따른 분류:**\n",
    "\n",
    "<center><img src='Image/Regression-Algorithms-Tree2.png' width='800'></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Boosting 모델\n",
    "\n",
    "- **Bagging과 Boosting 차이**\n",
    "\n",
    "<center><img src='Image/Bagging_Boosting.png' width='700'></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging 알고리즘\n",
    "\n",
    "- **의사결정나무(Decision Tree):**  \n",
    "\n",
    "<center><img src='Image/Bagging_DT.png' width='700'></center> \n",
    "\n",
    "- **렌덤포레스트(Random Forest):** 의사결정나무의 CLT버전\n",
    "\n",
    "<center><img src='Image/Bagging_RF.jpg' width='700'></center> \n",
    "\n",
    "~~~\n",
    "# DecisionTree\n",
    "fit = DecisionTreeRegressor().fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# RandomForestRegressor\n",
    "fit = RandomForestRegressor(n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting 알고리즘\n",
    "\n",
    "- **Gradient Boosting Machine(GBM):** 정규화된 GBM이라고하며, GMB보다 10배 빠른 알고리즘  \n",
    "\n",
    "<center><img src='Image/Boosting_GBM.jpg' width='600'></center> \n",
    "\n",
    "- **XGBoost(eXtreme Gradient Boosting):** 높은 예측력으로 많은 양의 데이터를 다룰 때 사용되는 부스팅 알고리즘  \n",
    "\n",
    "<center><img src='Image/Boosting_XGBoost.png' width='600'></center> \n",
    "\n",
    "- **LightGBM:** 현존하는 부스팅 알고리즘 중 가장 빠르고 높은 예측력 제공\n",
    "\n",
    "<center><img src='Image/Boosting_LightGBM.png' width='600'></center> \n",
    "\n",
    "| Algorithms | Specification | Others |\n",
    "|------------|-------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n",
    "| AdaBoost | 다수결을 통한 정답분류 및 오답에 가중치 부여 | - |\n",
    "| GBM | 손실함수(검증지표)의 Gradient로 오답에 가중치 부여 | - |\n",
    "| XGBoost | GMB대비 성능향상<br/>시스템(CPU, Mem.) 자원 효율적 사용 | 2014년 공개 |\n",
    "| LightGBM | XGBoost대비 성능향상 및 자원소모 최소화<br/>XGBoost가 처리하지 못하는 대용량 데이터 학습가능<br/>근사치분할(Approximates the Split)을 통한 성능향상 | 2016년 공개 |\n",
    "\n",
    "~~~\n",
    "# GradientBoostingRegression\n",
    "fit = GradientBoostingRegressor(alpha=0.1, learning_rate=0.05, loss='huber', criterion='friedman_mse',\n",
    "                                           n_estimators=1000, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "fit = XGBRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "\n",
    "# LightGMB\n",
    "fit = LGBMRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train, Y_train)\n",
    "pred_tr = fit.predict(X_train)\n",
    "pred_te = fit.predict(X_test)\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
